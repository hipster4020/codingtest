{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c57aa4",
   "metadata": {},
   "source": [
    "다대다 RNN은 대표적으로 품사 태깅, 개체명 인식 등에서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e8409",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f60541",
   "metadata": {},
   "source": [
    "# *Char RNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed02c3",
   "metadata": {},
   "source": [
    "RNN 입출력 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 <b>문자 단위 RNN</b>라고 함.<br>\n",
    "<u>RNN 구조 자체가 달라진 것은 아니고, 입,출력의 단위가 문자로 바뀜</u><br><br>\n",
    "<b>문자 단위 RNN을 다대다 구조</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a762d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871106c",
   "metadata": {},
   "source": [
    "### *processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d8c5b",
   "metadata": {},
   "source": [
    "문자 시퀀스 apple을 입력받아 pple!를 출력하는 RNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d90e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37913fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = sorted(list(set(input_str + label_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e477c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e396ea",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7a3e4",
   "metadata": {},
   "source": [
    "# *Encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4024da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_to_index : {'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "index_to_char : {0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = {v: k for k, v in enumerate(char_vocab)}\n",
    "index_to_char = {k: v for k, v in enumerate(char_vocab)}\n",
    "print(f\"char_to_index : {char_to_index}\")\n",
    "print(f\"index_to_char : {index_to_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be94dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[i] for i in input_str]\n",
    "y_data = [char_to_index[i] for i in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8c4be",
   "metadata": {},
   "source": [
    "### *batch 차원 추가*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d1f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.LongTensor(x_data)\n",
    "x_data = F.one_hot(x_data, num_classes=vocab_size) # one-hot encoding (LongTensor 필요)\n",
    "X = x_data.unsqueeze(0).type(torch.FloatTensor)\n",
    "Y = torch.LongTensor(y_data).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed11354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.]]])\n",
      "tensor([[4, 4, 3, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ce70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기 : torch.Size([1, 5, 5])\n",
      "레이블 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(f\"훈련 데이터 크기 : {X.shape}\")\n",
    "print(f\"레이블 크기 : {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5809a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c56f7",
   "metadata": {},
   "source": [
    "# *Train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ced327",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07b14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size, hidden_size, output_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d322231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2175, -0.2541,  0.5675, -0.0843,  0.1576],\n",
      "         [-0.0720, -0.2235,  0.3873, -0.4740,  0.5187],\n",
      "         [-0.1215, -0.1076,  0.4717, -0.3196,  0.3633],\n",
      "         [-0.3617, -0.2319,  0.7496, -0.3049,  0.4235],\n",
      "         [ 0.0344,  0.1267,  0.7013, -0.5676,  0.5601]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f527a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2175, -0.2541,  0.5675, -0.0843,  0.1576],\n",
      "        [-0.0720, -0.2235,  0.3873, -0.4740,  0.5187],\n",
      "        [-0.1215, -0.1076,  0.4717, -0.3196,  0.3633],\n",
      "        [-0.3617, -0.2319,  0.7496, -0.3049,  0.4235],\n",
      "        [ 0.0344,  0.1267,  0.7013, -0.5676,  0.5601]], grad_fn=<ViewBackward>)\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, input_size)) # 2차원 텐서로 변환\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa544314",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32ecb2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.52280592918396 prediction:  [[2 4 2 2 2]] true Y:  [4, 4, 3, 2, 0] prediction str:  epeee\n",
      "1 loss:  1.2713689804077148 prediction:  [[4 4 4 4 4]] true Y:  [4, 4, 3, 2, 0] prediction str:  ppppp\n",
      "2 loss:  1.073270559310913 prediction:  [[4 4 3 4 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pplp!\n",
      "3 loss:  0.8618305325508118 prediction:  [[4 4 3 4 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pplp!\n",
      "4 loss:  0.6540454030036926 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "5 loss:  0.47488659620285034 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "6 loss:  0.34092622995376587 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "7 loss:  0.2342413365840912 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "8 loss:  0.15783968567848206 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "9 loss:  0.10820503532886505 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "10 loss:  0.07014058530330658 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "11 loss:  0.04623717814683914 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "12 loss:  0.03258679062128067 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "13 loss:  0.024125967174768448 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "14 loss:  0.01829107478260994 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "15 loss:  0.01401529647409916 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "16 loss:  0.010828951373696327 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "17 loss:  0.008461912162601948 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "18 loss:  0.006715087685734034 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "19 loss:  0.005428635515272617 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "20 loss:  0.004475432448089123 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "21 loss:  0.0037583024241030216 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "22 loss:  0.003206582274287939 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "23 loss:  0.0027707740664482117 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "24 loss:  0.002417613286525011 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "25 loss:  0.0021252187434583902 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "26 loss:  0.0018795287469401956 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "27 loss:  0.0016713060904294252 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "28 loss:  0.001494462019763887 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "29 loss:  0.0013443285133689642 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "30 loss:  0.0012173731811344624 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "31 loss:  0.0011103933211416006 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "32 loss:  0.0010205407161265612 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "33 loss:  0.0009452259400859475 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "34 loss:  0.0008820483344607055 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "35 loss:  0.0008290584082715213 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "36 loss:  0.00078442448284477 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "37 loss:  0.000746623903978616 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "38 loss:  0.0007144190603867173 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "39 loss:  0.0006867626216262579 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "40 loss:  0.000662726117298007 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "41 loss:  0.0006417382974177599 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "42 loss:  0.0006232036976143718 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "43 loss:  0.0006067415233701468 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "44 loss:  0.0005918277893215418 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "45 loss:  0.000578295614104718 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "46 loss:  0.0005660738097503781 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "47 loss:  0.0005547095788642764 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "48 loss:  0.0005442982655949891 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "49 loss:  0.0005346254911273718 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "50 loss:  0.0005256436998024583 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "51 loss:  0.0005173526587896049 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "52 loss:  0.0005095858359709382 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "53 loss:  0.0005024384590797126 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "54 loss:  0.0004957198398187757 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "55 loss:  0.00048943015281111 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "56 loss:  0.0004835929721593857 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "57 loss:  0.00047816080041229725 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "58 loss:  0.0004731336666736752 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "59 loss:  0.00046836858382448554 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "60 loss:  0.0004638893879018724 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "61 loss:  0.0004597437218762934 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "62 loss:  0.0004557408974505961 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "63 loss:  0.0004520240763667971 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "64 loss:  0.0004484501259867102 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "65 loss:  0.0004450429114513099 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "66 loss:  0.0004417786840349436 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "67 loss:  0.0004386335494928062 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "68 loss:  0.0004356313147582114 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "69 loss:  0.00043272445327602327 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "70 loss:  0.00042986514745280147 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "71 loss:  0.00042705348460003734 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "72 loss:  0.0004244086449034512 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "73 loss:  0.0004217637761030346 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "74 loss:  0.00041919032810255885 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "75 loss:  0.00041671222425065935 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "76 loss:  0.00041428179247304797 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "77 loss:  0.00041182743734680116 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "78 loss:  0.00040953996358439326 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "79 loss:  0.0004072047886438668 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "80 loss:  0.0004049172275699675 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "81 loss:  0.00040272498154081404 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "82 loss:  0.00040055657154880464 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "83 loss:  0.0003984596114605665 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "84 loss:  0.0003963865456171334 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "85 loss:  0.0003943371993955225 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "86 loss:  0.0003922879113815725 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "87 loss:  0.0003903577453456819 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "88 loss:  0.00038837993633933365 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "89 loss:  0.0003864974423777312 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "90 loss:  0.0003846149193122983 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "91 loss:  0.00038275623228400946 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "92 loss:  0.00038094521733000875 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "93 loss:  0.00037915800930932164 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "94 loss:  0.00037732310011051595 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "95 loss:  0.00037553589208982885 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "96 loss:  0.0003737963561434299 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "97 loss:  0.0003720806271303445 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "98 loss:  0.000370317226042971 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n",
      "99 loss:  0.0003686253330670297 prediction:  [[4 4 3 2 0]] true Y:  [4, 4, 3, 2, 0] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b7e13",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268b2b0",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84d0f2",
   "metadata": {},
   "source": [
    "# *Char RNN(문장 단위 RNN)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c065f4",
   "metadata": {},
   "source": [
    "### *dictionary 정의*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d90617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474f316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"배를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57ffd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'시': 0, '도': 1, '경': 2, '대': 3, '치': 4, '할': 5, '가': 6, '나': 7, '하': 8, '배': 9, '르': 10, '이': 11, ' ': 12, '만': 13, '모': 14, '과': 15, '다': 16, '람': 17, '를': 18, '바': 19, '일': 20, '말': 21, '지': 22, '고': 23, '사': 24, '록': 25, '십': 26, '무': 27, '으': 28, '당': 29, '광': 30, '들': 31, '면': 32, '싶': 33, '없': 34, '을': 35, '업': 36, '오': 37, '작': 38, '한': 39, '동': 40, '.': 41, '거': 42, '아': 43, '끝': 44}\n"
     ]
    }
   ],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c839fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 45\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae74c04",
   "metadata": {},
   "source": [
    "### *one-hot encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40908975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10  # 임의 숫자 지정\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb761216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 배를 만들고 싶다면 -> 를 만들고 싶다면 \n",
      "1 를 만들고 싶다면  ->  만들고 싶다면 사\n",
      "2  만들고 싶다면 사 -> 만들고 싶다면 사람\n",
      "3 만들고 싶다면 사람 -> 들고 싶다면 사람들\n",
      "4 들고 싶다면 사람들 -> 고 싶다면 사람들을\n",
      "5 고 싶다면 사람들을 ->  싶다면 사람들을 \n",
      "6  싶다면 사람들을  -> 싶다면 사람들을 모\n",
      "7 싶다면 사람들을 모 -> 다면 사람들을 모아\n",
      "8 다면 사람들을 모아 -> 면 사람들을 모아 \n",
      "9 면 사람들을 모아  ->  사람들을 모아 나\n",
      "10  사람들을 모아 나 -> 사람들을 모아 나무\n",
      "11 사람들을 모아 나무 -> 람들을 모아 나무를\n",
      "12 람들을 모아 나무를 -> 들을 모아 나무를 \n",
      "13 들을 모아 나무를  -> 을 모아 나무를 모\n",
      "14 을 모아 나무를 모 ->  모아 나무를 모으\n",
      "15  모아 나무를 모으 -> 모아 나무를 모으거\n",
      "16 모아 나무를 모으거 -> 아 나무를 모으거나\n",
      "17 아 나무를 모으거나 ->  나무를 모으거나 \n",
      "18  나무를 모으거나  -> 나무를 모으거나 작\n",
      "19 나무를 모으거나 작 -> 무를 모으거나 작업\n",
      "20 무를 모으거나 작업 -> 를 모으거나 작업과\n",
      "21 를 모으거나 작업과 ->  모으거나 작업과 \n",
      "22  모으거나 작업과  -> 모으거나 작업과 일\n",
      "23 모으거나 작업과 일 -> 으거나 작업과 일을\n",
      "24 으거나 작업과 일을 -> 거나 작업과 일을 \n",
      "25 거나 작업과 일을  -> 나 작업과 일을 할\n",
      "26 나 작업과 일을 할 ->  작업과 일을 할당\n",
      "27  작업과 일을 할당 -> 작업과 일을 할당하\n",
      "28 작업과 일을 할당하 -> 업과 일을 할당하지\n",
      "29 업과 일을 할당하지 -> 과 일을 할당하지 \n",
      "30 과 일을 할당하지  ->  일을 할당하지 말\n",
      "31  일을 할당하지 말 -> 일을 할당하지 말고\n",
      "32 일을 할당하지 말고 -> 을 할당하지 말고 \n",
      "33 을 할당하지 말고  ->  할당하지 말고 끝\n",
      "34  할당하지 말고 끝 -> 할당하지 말고 끝없\n",
      "35 할당하지 말고 끝없 -> 당하지 말고 끝없이\n",
      "36 당하지 말고 끝없이 -> 하지 말고 끝없이 \n",
      "37 하지 말고 끝없이  -> 지 말고 끝없이 광\n",
      "38 지 말고 끝없이 광 ->  말고 끝없이 광대\n",
      "39  말고 끝없이 광대 -> 말고 끝없이 광대한\n",
      "40 말고 끝없이 광대한 -> 고 끝없이 광대한 \n",
      "41 고 끝없이 광대한  ->  끝없이 광대한 바\n",
      "42  끝없이 광대한 바 -> 끝없이 광대한 바다\n",
      "43 끝없이 광대한 바다 -> 없이 광대한 바다를\n",
      "44 없이 광대한 바다를 -> 이 광대한 바다를 \n",
      "45 이 광대한 바다를  ->  광대한 바다를 동\n",
      "46  광대한 바다를 동 -> 광대한 바다를 동경\n",
      "47 광대한 바다를 동경 -> 대한 바다를 동경하\n",
      "48 대한 바다를 동경하 -> 한 바다를 동경하도\n",
      "49 한 바다를 동경하도 ->  바다를 동경하도록\n",
      "50  바다를 동경하도록 -> 바다를 동경하도록 \n",
      "51 바다를 동경하도록  -> 다를 동경하도록 가\n",
      "52 다를 동경하도록 가 -> 를 동경하도록 가르\n",
      "53 를 동경하도록 가르 ->  동경하도록 가르치\n",
      "54  동경하도록 가르치 -> 동경하도록 가르치십\n",
      "55 동경하도록 가르치십 -> 경하도록 가르치십시\n",
      "56 경하도록 가르치십시 -> 하도록 가르치십시오\n",
      "57 하도록 가르치십시오 -> 도록 가르치십시오.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7475cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 18, 12, 13, 31, 23, 12, 33, 16, 32]\n",
      "[18, 12, 13, 31, 23, 12, 33, 16, 32, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0]) # 배를 만들고 싶다면\n",
    "print(y_data[0]) # 를 만들고 싶다면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee72bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "# X = torch.FloatTensor(x_one_hot)\n",
    "# Y = torch.LongTensor(y_data)\n",
    "x_data = F.one_hot(torch.LongTensor(x_data), num_classes=dic_size) # one-hot encoding (LongTensor 필요)\n",
    "X = x_data.type(torch.FloatTensor)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658f4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([58, 10, 45])\n",
      "Y shape : torch.Size([58, 10])\n",
      "X : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Y : tensor([[18, 12, 13, 31, 23, 12, 33, 16, 32, 12],\n",
      "        [12, 13, 31, 23, 12, 33, 16, 32, 12, 24],\n",
      "        [13, 31, 23, 12, 33, 16, 32, 12, 24, 17],\n",
      "        [31, 23, 12, 33, 16, 32, 12, 24, 17, 31],\n",
      "        [23, 12, 33, 16, 32, 12, 24, 17, 31, 35],\n",
      "        [12, 33, 16, 32, 12, 24, 17, 31, 35, 12],\n",
      "        [33, 16, 32, 12, 24, 17, 31, 35, 12, 14],\n",
      "        [16, 32, 12, 24, 17, 31, 35, 12, 14, 43],\n",
      "        [32, 12, 24, 17, 31, 35, 12, 14, 43, 12],\n",
      "        [12, 24, 17, 31, 35, 12, 14, 43, 12,  7],\n",
      "        [24, 17, 31, 35, 12, 14, 43, 12,  7, 27],\n",
      "        [17, 31, 35, 12, 14, 43, 12,  7, 27, 18],\n",
      "        [31, 35, 12, 14, 43, 12,  7, 27, 18, 12],\n",
      "        [35, 12, 14, 43, 12,  7, 27, 18, 12, 14],\n",
      "        [12, 14, 43, 12,  7, 27, 18, 12, 14, 28],\n",
      "        [14, 43, 12,  7, 27, 18, 12, 14, 28, 42],\n",
      "        [43, 12,  7, 27, 18, 12, 14, 28, 42,  7],\n",
      "        [12,  7, 27, 18, 12, 14, 28, 42,  7, 12],\n",
      "        [ 7, 27, 18, 12, 14, 28, 42,  7, 12, 38],\n",
      "        [27, 18, 12, 14, 28, 42,  7, 12, 38, 36],\n",
      "        [18, 12, 14, 28, 42,  7, 12, 38, 36, 15],\n",
      "        [12, 14, 28, 42,  7, 12, 38, 36, 15, 12],\n",
      "        [14, 28, 42,  7, 12, 38, 36, 15, 12, 20],\n",
      "        [28, 42,  7, 12, 38, 36, 15, 12, 20, 35],\n",
      "        [42,  7, 12, 38, 36, 15, 12, 20, 35, 12],\n",
      "        [ 7, 12, 38, 36, 15, 12, 20, 35, 12,  5],\n",
      "        [12, 38, 36, 15, 12, 20, 35, 12,  5, 29],\n",
      "        [38, 36, 15, 12, 20, 35, 12,  5, 29,  8],\n",
      "        [36, 15, 12, 20, 35, 12,  5, 29,  8, 22],\n",
      "        [15, 12, 20, 35, 12,  5, 29,  8, 22, 12],\n",
      "        [12, 20, 35, 12,  5, 29,  8, 22, 12, 21],\n",
      "        [20, 35, 12,  5, 29,  8, 22, 12, 21, 23],\n",
      "        [35, 12,  5, 29,  8, 22, 12, 21, 23, 12],\n",
      "        [12,  5, 29,  8, 22, 12, 21, 23, 12, 44],\n",
      "        [ 5, 29,  8, 22, 12, 21, 23, 12, 44, 34],\n",
      "        [29,  8, 22, 12, 21, 23, 12, 44, 34, 11],\n",
      "        [ 8, 22, 12, 21, 23, 12, 44, 34, 11, 12],\n",
      "        [22, 12, 21, 23, 12, 44, 34, 11, 12, 30],\n",
      "        [12, 21, 23, 12, 44, 34, 11, 12, 30,  3],\n",
      "        [21, 23, 12, 44, 34, 11, 12, 30,  3, 39],\n",
      "        [23, 12, 44, 34, 11, 12, 30,  3, 39, 12],\n",
      "        [12, 44, 34, 11, 12, 30,  3, 39, 12, 19],\n",
      "        [44, 34, 11, 12, 30,  3, 39, 12, 19, 16],\n",
      "        [34, 11, 12, 30,  3, 39, 12, 19, 16, 18],\n",
      "        [11, 12, 30,  3, 39, 12, 19, 16, 18, 12],\n",
      "        [12, 30,  3, 39, 12, 19, 16, 18, 12, 40],\n",
      "        [30,  3, 39, 12, 19, 16, 18, 12, 40,  2],\n",
      "        [ 3, 39, 12, 19, 16, 18, 12, 40,  2,  8],\n",
      "        [39, 12, 19, 16, 18, 12, 40,  2,  8,  1],\n",
      "        [12, 19, 16, 18, 12, 40,  2,  8,  1, 25],\n",
      "        [19, 16, 18, 12, 40,  2,  8,  1, 25, 12],\n",
      "        [16, 18, 12, 40,  2,  8,  1, 25, 12,  6],\n",
      "        [18, 12, 40,  2,  8,  1, 25, 12,  6, 10],\n",
      "        [12, 40,  2,  8,  1, 25, 12,  6, 10,  4],\n",
      "        [40,  2,  8,  1, 25, 12,  6, 10,  4, 26],\n",
      "        [ 2,  8,  1, 25, 12,  6, 10,  4, 26,  0],\n",
      "        [ 8,  1, 25, 12,  6, 10,  4, 26,  0, 37],\n",
      "        [ 1, 25, 12,  6, 10,  4, 26,  0, 37, 41]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape : {X.shape}\")\n",
    "print(f\"Y shape : {Y.shape}\")\n",
    "print(f\"X : {X}\")\n",
    "print(f\"Y : {Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219fc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d41af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 12, 13, 31, 23, 12, 33, 16, 32, 12])\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e650229",
   "metadata": {},
   "source": [
    "### *train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de229aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True) # num_layer : 은닉층 갯수\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c4187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a1b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf68aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 10, 45])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "051ecdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 10, 45])\n",
      "tensor([[[ 0.1784,  0.2366,  0.0113,  ..., -0.0373,  0.2178,  0.0005],\n",
      "         [ 0.2868,  0.1274, -0.1131,  ..., -0.1049,  0.0895,  0.0255],\n",
      "         [ 0.3012,  0.2528, -0.0762,  ..., -0.1812,  0.1059,  0.0378],\n",
      "         ...,\n",
      "         [ 0.3040,  0.2580, -0.1603,  ..., -0.1550,  0.2047,  0.1423],\n",
      "         [ 0.3755,  0.2062, -0.1192,  ..., -0.1115,  0.1821,  0.0052],\n",
      "         [ 0.3191,  0.2543, -0.1698,  ..., -0.1186,  0.1224,  0.0527]],\n",
      "\n",
      "        [[ 0.2252,  0.1885, -0.0836,  ..., -0.0336,  0.1587,  0.0039],\n",
      "         [ 0.2831,  0.2169, -0.0421,  ..., -0.1862,  0.1293,  0.0233],\n",
      "         [ 0.3589,  0.2465, -0.0999,  ..., -0.0718,  0.1473,  0.0478],\n",
      "         ...,\n",
      "         [ 0.3768,  0.2076, -0.1190,  ..., -0.1110,  0.1802,  0.0051],\n",
      "         [ 0.3170,  0.2533, -0.1695,  ..., -0.1195,  0.1216,  0.0524],\n",
      "         [ 0.3813,  0.2951, -0.1023,  ..., -0.1989,  0.1630, -0.0227]],\n",
      "\n",
      "        [[ 0.2444,  0.2313, -0.0337,  ..., -0.1034,  0.1502, -0.0589],\n",
      "         [ 0.2828,  0.2457, -0.0845,  ..., -0.0857,  0.1627,  0.0054],\n",
      "         [ 0.2810,  0.2308, -0.0829,  ..., -0.0532,  0.1388,  0.0642],\n",
      "         ...,\n",
      "         [ 0.3177,  0.2547, -0.1693,  ..., -0.1167,  0.1194,  0.0535],\n",
      "         [ 0.3805,  0.2948, -0.1021,  ..., -0.2002,  0.1610, -0.0225],\n",
      "         [ 0.3181,  0.2467, -0.1466,  ..., -0.0944,  0.1497,  0.0783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1989,  0.2069, -0.0414,  ..., -0.0264,  0.1952, -0.0439],\n",
      "         [ 0.2068,  0.2369, -0.0956,  ..., -0.1281,  0.1289,  0.0393],\n",
      "         [ 0.2926,  0.1967, -0.0386,  ..., -0.0841,  0.1374,  0.0965],\n",
      "         ...,\n",
      "         [ 0.3188,  0.2557, -0.1308,  ..., -0.0512,  0.1646,  0.0641],\n",
      "         [ 0.2945,  0.2175, -0.1364,  ..., -0.1537,  0.0902,  0.1268],\n",
      "         [ 0.3074,  0.2157, -0.1266,  ..., -0.1083,  0.1670,  0.0221]],\n",
      "\n",
      "        [[ 0.1787,  0.2359, -0.0428,  ..., -0.0330,  0.1736,  0.0053],\n",
      "         [ 0.2420,  0.1901, -0.0029,  ..., -0.1094,  0.1565,  0.0651],\n",
      "         [ 0.3187,  0.1764, -0.0615,  ..., -0.0910,  0.1025,  0.1038],\n",
      "         ...,\n",
      "         [ 0.2956,  0.2187, -0.1364,  ..., -0.1525,  0.0878,  0.1276],\n",
      "         [ 0.3060,  0.2152, -0.1269,  ..., -0.1099,  0.1662,  0.0222],\n",
      "         [ 0.1874,  0.2016, -0.1447,  ..., -0.0799,  0.2025,  0.1281]],\n",
      "\n",
      "        [[ 0.1776,  0.2120, -0.0273,  ..., -0.0171,  0.1894,  0.0113],\n",
      "         [ 0.2735,  0.1771, -0.0285,  ..., -0.1294,  0.1203,  0.0556],\n",
      "         [ 0.3262,  0.2711, -0.1235,  ..., -0.1011,  0.1218,  0.1229],\n",
      "         ...,\n",
      "         [ 0.3071,  0.2169, -0.1260,  ..., -0.1080,  0.1630,  0.0224],\n",
      "         [ 0.1852,  0.2006, -0.1443,  ..., -0.0816,  0.2006,  0.1276],\n",
      "         [ 0.4079,  0.2148, -0.1094,  ..., -0.1278,  0.1958,  0.0807]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0ead39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([580, 45])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8c8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 10])\n",
      "torch.Size([580])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3db53b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02cd9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도시대시시도시시시시시시시시시시시시대시시시시시시시시시시대시시대시대시대대시시시도시도대시대시시시시시시시시시도시시시대시시시시대시\n",
      "                                                                   \n",
      "를 모 을 가 을  당을당을 당    을 을  을 을당  을을 을 을 을  을당을  을고 을당  을당  을  당당 을  \n",
      "                                                                   \n",
      "  말대거 모대거  거거   모       거  거        거   거  거    거 말거  말거 모 거   거  모\n",
      "면할광람도모작고도록록 나치도나람없없나 나과고나없들나나과들나 들고고 광들도나과들나 광과고 없록나광과들 할한도나광고고나나 광\n",
      "면 가다면 대아 바아  작이 당일  이 일이 당아  일이 이 모이  일이 록다들  일이 모만  광아면 록아  일이    \n",
      "를 싶다를 할다를 할다  아치  할다다  아를 모다  할람경  아를  아를 싶람를  경를 할다를 할다를 할과를 아거아과 \n",
      "를 사다  모경하 모경하 모없  모없다  사하 모경를 싶없다  사다  록다 모없다  경하 모다하 모없하 모없하   없없 \n",
      "를 사하  바없하 바고하   하 모고   사하  없을  없   사하  바하  없하  없하  대하 바없하  없하   없을 \n",
      "를 바람   고들 바고들 바을고  고   고   을을  무을  무   바고  고업  고  바고들 바람들 바을업   고  \n",
      "를 바대이 끝다들 바람들  무고  무업  나이  무이  무업  당업  록업  을업  다이 바대들 동다이  없업들  바이 \n",
      "를 바대이 끝다를 동다를도 록다  당다  을를도 다으  록다  바다  록다  다이과끝다를 동다를 동다를  다다를과 당이 \n",
      "를 사을면 광을면 당다를  일를  당이 광을  당을업면 당을  당를과 사다  다를 일다과  다면 사다를 록다하를  당를 \n",
      "를 사다하 사경하 사경하  말고 할없람 모경  당없르면 당다  없하치 말고 끝다하 모경  사경하 사경하 록말하를 할말고 \n",
      "를 바다하 모경하 모경하도 말고  없고  아   없업르 말업  없하치 말고 할없하 모경하 모경하 모경하도록말경고치 말고 \n",
      "를 바람하 모대한 바다하  말아  무르  아한  업르르 말업  무르치 말고  무하  다하 싶대를 동다하도록말대르  말람 \n",
      "를 바다를 동다를 동다를도록말람  무를 끝다를  당을  바가 끝당고도록말고  당이 바다를 동다를 동다를도록당가고  말고 \n",
      "를 바다를 바다를 바고를나 동다  당과 바다하나 광업과 광업 끝당이지 말다 끝당이 바다를 바다를 바다하도록 말고  시이 \n",
      "를 사다하 모다를 광다하나 일을 끝당과 모다하나 작업거 일업 나업하지 일고 끝당하 광을하나 다를 싶없하도록 말고 나시  \n",
      "를 사다를 사경를 사고 나 말고  없하 사경하  일을  말을 할없하  일고 모다하 사경를 사경를 사람를도록말가하 십없오 \n",
      "한 사경하 사경를 사경하도 사을 할무하 모경하 모경고  일다  없하도 다오 모경하 모경를 사경를 사경하 모 업르 싶경오 \n",
      "한 모다들 싶다를 동다들도 바대들나무하 모대거을 작을치 모을 나무하도 모을거나무하 모다  동다들 동다들도록 가르치십말업치\n",
      "를 광아한을 다를지광다를을 작을 나대과  대과을 작업치 말업 나당과을 작업 나무과  대   다를지 다를도록 작르치 말  \n",
      "를 바아  바다를 바다를을 작을 끝당를 모다 을 작업  말을 끝없를지 작르 끝없과 광아  바다를 동다를지록 작르 십시  \n",
      "를 바다를 동다를 동다를도 바다 나당이 동다를나 작대  말고 끝없이지 말고 끝당이 동다를 바다를 동다를도록 가고 십시이.\n",
      "를 모다면 모다하 동다하을 모다 나당이 모다하나 작업  일고 끝당이도 말고 끝당이 모다면 바다하 동경하도록 가르 십시이.\n",
      "를 모으들 싶으들 사으하을 모으 할무하 모으들나무작업  일대 모당하도 말고 할무이 모으들 사으들 사경하도록 가르도십시오.\n",
      "를 사아거 싶경면 사으들나 말대 할무하 사으들나 작업과 일을 할무하지 말고 할무이 모경거 사경한 사경하도록 가르 십시오도\n",
      "를 사람거 동경를 사람하을 말하 할무를 모람거나 작업과 일을 끝없하지십말고 끝없이 모람거 동경를 동경하도록 가르치십시오.\n",
      "를 모아  동다를 바람를나 광으거나무를 광다거나 작업과 일을 끝당를지 말고 끝없이 일다거 바람를 동다하도록 가르치십시오.\n",
      "를 모아  동다를 바다하을 모으 끝무하 모다 나 작업과 일을 끝당하지 말고 끝없이 모다거 바다를 동다하도록 가르치십시오.\n",
      "를 모아  동다를 바다하을 모아 나무하 모아거나 작업과 일을 나당하지 가고 끝없이 모다면 바다를 동다하지록 가르치십시오.\n",
      "를 모아거 동경면 바람하을 모으 나무이 모으거나 작업과 모을 나당이지 말고 끝없이 광아한 바경면 동경하도록 가르치십시오.\n",
      "를 모람거 동경면 바람들을 모으 나무이 모으거나 작업과 일을 할당이지 말고 끝없이 모아한 바경면 동경하도록 가르치십시오.\n",
      "를 모람거 싶경를 사람들을 모아 할무를 모으거나 작업과 일을 할당하지 말고 끝없이 광아한 바경를 동경들도록 가르치십시오 \n",
      "를 모다한 싶경를 사람들을 모아 할무를 모아거나 작업과 일을 나당하지 말고 끝없이 광아한 바경를 동경들도록 가르치십시오.\n",
      "를 모다한 싶다를 사다들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광아한 바다를 동다하도록 가르치십시오.\n",
      "를 모다한 싶다면 사다들을 모아 나무를 모으거나 작업과 일을 끝당하지 말고 끝없이 광대한 바다를 동다하도록 가르치십시오.\n",
      "를 모으한 동다면 사람하을 모아 나무를 모으거나 작업과 일을 할대하지 말고 끝없이 광대한 바람면 동다하도록 가르치십시오.\n",
      "를 모으한 동경면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광으한 바경를 동경하도록 가르치십시오.\n",
      "를 모으한 싶경면 사람들을 모아 나무를 모아거나 작업과 일을 할당하지 말고 끝없이 광대한 바람를 동경하도록 가르치십시오.\n",
      "를 모들한 싶다면 바람들을 모아 나무를 모아거나 작업과 일을 할당하지 말고 끝없이 광대한 바람를 동경하도록 가르치십시오.\n",
      "를 모들를 싶다를 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광으한 바경를 동경하도록 가르치십시오.\n",
      "를 만들면 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동다하도록 가르치십시오.\n",
      "를 만들면 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동다하도록 가르치십시오.\n",
      "를 만들한 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동다하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바경를 동경하도록 가르치십시오.\n",
      "를 모들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바경를 동경하도록 가르치십시오.\n",
      "를 모들한 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바경를 동경하도록 가르치십시오.\n",
      "를 만들한 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동다하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n",
      "를 만들고 싶다면 사람들을 모아 나무를 모으거나 작업과 일을 할당하지 말고 끝없이 광대한 바다를 동경하도록 가르치십시오.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1085e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
