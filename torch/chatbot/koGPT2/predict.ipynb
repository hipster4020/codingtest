{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9741182a-9ed1-497d-be9a-7e538ef41ae7",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc9e0d1-079a-4f96-bd6e-fbc219d35409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ec40bc-8e70-4089-8502-8c7265626eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b3f694-1712-4f74-8545-3a482f7b4df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅÏïàÎÖï',\n",
       " 'Ìïò',\n",
       " 'ÏÑ∏',\n",
       " 'Ïöî.',\n",
       " '‚ñÅÌïúÍµ≠Ïñ¥',\n",
       " '‚ñÅG',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '‚ñÅÏûÖ',\n",
       " 'ÎãàÎã§.',\n",
       " 'üò§',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"ÏïàÎÖïÌïòÏÑ∏Ïöî. ÌïúÍµ≠Ïñ¥ GPT-2 ÏûÖÎãàÎã§.üò§:)l^o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dd1e9-3184-4625-bef6-8db35cea398f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca0d43b-839c-467e-b16d-c191a1753cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100e0c79-1f85-40fd-adb0-9c984d4af9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoConfig.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1475e67b-dbde-4db1-b4fb-dafea9bcc7bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0708f131826b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Í∑ºÏú°Ïù¥ Ïª§ÏßÄÍ∏∞ ÏúÑÌï¥ÏÑúÎäî'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m gen_ids = model.generate(torch.tensor([input_ids]),\n\u001b[1;32m      4\u001b[0m                            \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = 'Í∑ºÏú°Ïù¥ Ïª§ÏßÄÍ∏∞ ÏúÑÌï¥ÏÑúÎäî'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27786592-364d-41fb-9757-18e780eea5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef978a-bfa3-483c-8174-be450454f795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
